{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\au616584\\OneDrive - Aarhus Universitet\\Datalogi\\PhD\\Faster-GPT-generation\\minGPT\\minGPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'minGPT' already exists and is not an empty directory.\n",
      "ERROR: file:///C:/Users/au616584/OneDrive%20-%20Aarhus%20Universitet/Datalogi/PhD/Faster-GPT-generation/minGPT/minGPT does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!git clone https://github.com/karpathy/minGPT.git  -q\n",
    "%cd minGPT\n",
    "!pip install -e . -qqq\n",
    "!pip install transformers[torch] -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step\n",
      "step 2\n",
      "step 3\n",
      "step 4\n",
      "number of parameters: 774.03M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 666/666 [00:00<00:00, 29.0kB/s]\n",
      "c:\\Users\\au616584\\Anaconda3\\envs\\minGPT\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\au616584\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 3.25G/3.25G [01:34<00:00, 34.5MB/s]\n",
      "Downloading data files:   0%|          | 0/2 [2:21:25<?, ?it/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 6.89kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (28): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (29): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (30): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (31): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (32): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (33): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (34): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (35): Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from mingpt.model import GPT\n",
    "print(\"step\")\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "print(\"step 2\")\n",
    "from mingpt.utils import set_seed\n",
    "print(\"step 3\")\n",
    "from mingpt.bpe import BPETokenizer\n",
    "set_seed(3407)\n",
    "print(\"step 4\")\n",
    "model_type = 'gpt2-large' \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT.from_pretrained(model_type)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mingpt.bpe import BPETokenizer\n",
    "tokenizer = BPETokenizer()\n",
    "\n",
    "num_samples = 1\n",
    "\n",
    "x   = torch.tensor([[tokenizer.encoder.encoder['Hi']]], dtype=torch.long)\n",
    "x   = x.expand(num_samples, -1).to(device)\n",
    "y   = model.generate(x, max_new_tokens=3, do_sample=True, top_k=40)\n",
    "out = tokenizer.decode(y[0].cpu().squeeze())\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiboys Theory\n",
      "torch.Size([1, 3, 50257]) torch.Size([1, 3])\n",
      "tensor([[0.88218599557876586914, 0.91500407457351684570],\n",
      "        [0.88228327035903930664, 0.91504842042922973633]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([0.88226926326751708984, 0.91500395536422729492])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "import torch as T\n",
    "\n",
    "def sample_tokens(logits: T.Tensor, z: T.Tensor) -> T.Tensor:\n",
    "    cdf = T.cumsum(T.nn.functional.softmax(logits, dim=1), dim=1)\n",
    "    return (cdf < z[:, None]).sum(1) # holy fucking douglas\n",
    "\n",
    "def unsample_z(logits: T.Tensor, tokens: T.Tensor) -> T.Tensor:\n",
    "    cdf = T.cumsum(T.cat([T.zeros_like(logits[:, 0, None]), T.nn.functional.softmax(logits, dim=1)], dim=1), dim=1)\n",
    "    idx = T.arange(tokens.shape[0])\n",
    "    return T.stack([cdf[idx, tokens], cdf[idx, tokens+1]])\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(self, idx, Z, max_new_tokens):\n",
    "\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits    = logits[:, -1, :] \n",
    "            idx_next  = sample_tokens(logits, Z[i].view(1,) ).view(1,1)\n",
    "\n",
    "            redo = unsample_z(logits, idx_next)\n",
    "            assert redo[0] <= Z[i] <=  redo[1]\n",
    "\n",
    "            idx       = torch.cat((idx, idx_next), dim=1)\n",
    "            #print(redo[0], Z[i], redo[1])\n",
    "\n",
    "        return idx\n",
    "\n",
    "# generate\n",
    "max_new_tokens = 2\n",
    "n = 50257\n",
    "\n",
    "torch.manual_seed(42)\n",
    "Z = torch.rand((max_new_tokens), device=device)\n",
    "\n",
    "num_samples = 1\n",
    "x = torch.tensor([[tokenizer.encoder.encoder['Hi']]], dtype=torch.long)\n",
    "x = x.expand(num_samples, -1).to(device)\n",
    "\n",
    "y = generate(model, x, Z, max_new_tokens=max_new_tokens)#, top_k=40)\n",
    "out = tokenizer.decode(y[0].cpu().squeeze())\n",
    "print(out)\n",
    "\n",
    "# y = generate(model, x, Z, max_new_tokens=max_new_tokens)#, top_k=40)\n",
    "# out = tokenizer.decode(y[0].cpu().squeeze())\n",
    "# print(out)\n",
    "\n",
    "logits, _ = model(y)\n",
    "print(logits.shape, y.shape)\n",
    "#idx_next  = sample_tokens(logits, Z.view(1,) ).view(1,1)\n",
    "\n",
    "logits, _ = model(y)\n",
    "redo = unsample_z(logits[0, 0:-1], y[0, 1:]) # this is the inverse!  F(logits, y) \n",
    "print(redo)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(11)\n",
    "# Z = torch.rand((max_new_tokens), device=device)\n",
    "# print(f\"Z_j = {Z[0]}\")\n",
    "# print(f\"Z = {Z}\")\n",
    "# x = torch.tensor([[tokenizer.encoder.encoder['Hi']]], dtype=torch.long)\n",
    "# print(x)\n",
    "# y_z = generate(model, x, Z, max_new_tokens=max_new_tokens)#, top_k=40)\n",
    "# print(y_z)\n",
    "# print(tokenizer.decode(y_z[0].cpu().squeeze()))\n",
    "# x_ = x.clone()\n",
    "# for p in range(max_new_tokens):\n",
    "#     for i in range(n):\n",
    "#         print(i, end='\\r')\n",
    "#         x_prime = torch.cat((x_, torch.tensor([[i]], dtype=torch.long)), dim=1)\n",
    "#         # x_prime = torch.cat((x_prime, torch.tensor([[i]], dtype=torch.long)), dim=1)\n",
    "#         logits, _ = model(x_prime)\n",
    "#         redo = unsample_z(logits[0, 0:-1], x_prime[0, 1:]) # this is the inverse!  F(logits, y)\n",
    "#         # print(redo[0], Z[p], redo[1][p],end='\\r')\n",
    "#         if redo[0][p] <= Z[p] <= redo[1][p]:\n",
    "#             print(i, redo[0], Z, redo[1])\n",
    "#             x_ = x_prime\n",
    "#             break\n",
    "#This is doing the same as sample tokens!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_j = 0.46568745374679565\n",
      "Z = tensor([0.46568745374679565430, 0.23276680707931518555])\n",
      "tensor([[ 1238,  5556,   352, 21767]])\n",
      "tensor([[ 1238,  5556,   352, 21767,   860,    13]])\n",
      "20 plus 1 equals 9.\n",
      "tensor([[860]])\n",
      "tensor([[13]])\n",
      "20 plus 1 equals 9.\n"
     ]
    }
   ],
   "source": [
    "#do the same, but iterate over everything in redo to find the right one\n",
    "torch.manual_seed(12)\n",
    "max_new_tokens = 2\n",
    "Z = torch.rand((max_new_tokens), device=device)\n",
    "print(f\"Z_j = {Z[0]}\")\n",
    "print(f\"Z = {Z}\")\n",
    "x = torch.tensor([tokenizer.encoder.encode('20 plus 1 equals')], dtype=torch.long)\n",
    "print(x)\n",
    "y_z = generate(model, x, Z, max_new_tokens=max_new_tokens)#, top_k=40)\n",
    "print(y_z)\n",
    "print(tokenizer.decode(y_z[0].cpu().squeeze()))\n",
    "torch.manual_seed(11)\n",
    "\n",
    "x_ = x.clone()\n",
    "for p in range(max_new_tokens):\n",
    "        # x_prime = torch.cat((x_prime, torch.tensor([[i]], dtype=torch.long)), dim=1)\n",
    "    logits, _ = model(x_)\n",
    "    next_word = sample_tokens(logits[:, -1, :], Z[p].view(1,) ).view(1,1)\n",
    "    print(next_word)\n",
    "    x_ = torch.cat((x_, next_word), dim=1)    \n",
    "        # redo = unsample_z(logits[0, 0:-1], x_prime[0, 1:]) # this is the inverse!  F(logits, y)\n",
    "        # # print(redo[0], Z[p], redo[1][p],end='\\r')\n",
    "        # if redo[0][p] <= Z[p] <= redo[1][p]:\n",
    "        #     print(i, redo[0], Z, redo[1])\n",
    "        #     x_ = x_prime\n",
    "        #     break\n",
    "print(tokenizer.decode(x_[0].cpu().squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_j = 0.14904171228408813\n",
      "Z = tensor([0.14904171228408813477])\n",
      "tensor([[17250]])\n",
      "550\r"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "Z = torch.rand((1), device=device)\n",
    "print(f\"Z_j = {Z[0]}\")\n",
    "print(f\"Z = {Z}\")\n",
    "x = torch.tensor([[tokenizer.encoder.encoder['Hi']]], dtype=torch.long)\n",
    "print(x)\n",
    "x_ = x.clone()\n",
    "for i in range(550,551):\n",
    "    print(i, end='\\r')\n",
    "    x_prime = torch.cat((x_, torch.tensor([[tokenizer.encoder.encoder['.']]], dtype=torch.long)), dim=1)\n",
    "    # x_prime = torch.cat((x_prime, torch.tensor([[i]], dtype=torch.long)), dim=1)\n",
    "    logits, _ = model(x_prime)\n",
    "    redo = unsample_z(logits[0, 0:-1], x_prime[0, 1:]) # this is the inverse!  F(logits, y)\n",
    "    # print(redo[0], Z[p], redo[1][p],end='\\r')\n",
    "    if redo[0] <= Z <= redo[1]:\n",
    "        print(i, redo[0], Z, redo[1])\n",
    "        x_ = x_prime\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([[17250,    11, 24977,   475,  6954,   286]])\n",
      "Hi, 1949 but evolution of\n",
      "torch.Size([1, 6, 50257])\n",
      "Z:tensor([0.18694752454757690430, 0.96132838726043701172, 0.68344897031784057617,\n",
      "        0.89879584312438964844, 0.05050849914550781250]), inverse range: tensor([[0.08008711040019989014, 0.96132808923721313477, 0.68332219123840332031,\n",
      "         0.89879155158996582031, 0.02872197888791561127],\n",
      "        [0.29271295666694641113, 0.96132868528366088867, 0.68355798721313476562,\n",
      "         0.89879667758941650391, 0.08397702127695083618]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Hi, Omar,\" proves a\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(45)\n",
    "\n",
    "\n",
    "max_new_tokens = 5\n",
    "n = 50257\n",
    "Z = torch.rand((max_new_tokens), device=device)\n",
    "\n",
    "num_samples = 1\n",
    "x = torch.tensor([[tokenizer.encoder.encoder['Hi']]], dtype=torch.long)\n",
    "x = x.expand(num_samples, -1).to(device)\n",
    "y = generate(model, x, Z, max_new_tokens=max_new_tokens)#, top_k=40)\n",
    "print(f\"y: {y}\")\n",
    "out = tokenizer.decode(y[0].cpu().squeeze())\n",
    "print(out)\n",
    "\n",
    "logits, _ = model(y) \n",
    "print(logits.shape)\n",
    "inverse_ranges = unsample_z(logits[0,0:-1], y[0,1:])\n",
    "assert(torch.all(torch.logical_and(inverse_ranges[0]<=Z, Z<=inverse_ranges[1])))\n",
    "print(f\"Z:{Z}, inverse range: {inverse_ranges}\")\n",
    "#Test that a different number in the interval is mapped to the same value\n",
    "Z_prime = inverse_ranges[0]+0.000001\n",
    "# print(Z_prime)\n",
    "y_prime = generate(model, x, Z_prime, max_new_tokens = max_new_tokens)\n",
    "out_prime = tokenizer.decode(y_prime[0].cpu().squeeze())\n",
    "print(out_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17250,    11, 24977,   475,  6954,   296]])\n",
      "Hi, 1949 but evolutionom\n",
      "tensor([[0.08008711040019989014, 0.96132808923721313477, 0.68332219123840332031,\n",
      "         0.89879155158996582031, 0.12114837020635604858],\n",
      "        [0.29271295666694641113, 0.96132868528366088867, 0.68355798721313476562,\n",
      "         0.89879667758941650391, 0.12114951014518737793]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[17250,    31, 24977,   475,  6954,   286]])\n",
      "Hi@ 1949 but evolution of\n",
      "tensor([[0.44990682601928710938, 0.83459746837615966797, 0.52469068765640258789,\n",
      "         0.89261054992675781250, 0.03333691507577896118],\n",
      "        [0.45155003666877746582, 0.83459752798080444336, 0.52479505538940429688,\n",
      "         0.89261370897293090820, 0.15026678144931793213]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[17250,    52, 24977,   475,  6954,   286]])\n",
      "HiU 1949 but evolution of\n",
      "tensor([[0.47763058543205261230, 0.95056873559951782227, 0.56360703706741333008,\n",
      "         0.88898706436157226562, 0.02912090905010700226],\n",
      "        [0.47830116748809814453, 0.95056933164596557617, 0.56399750709533691406,\n",
      "         0.88899099826812744141, 0.19175752997398376465]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[17250,    74, 24977,   475,  6954,   286]])\n",
      "Hik 1949 but evolution of\n",
      "tensor([[0.50090765953063964844, 0.93645834922790527344, 0.61492502689361572266,\n",
      "         0.88828891515731811523, 0.02305196970701217651],\n",
      "        [0.50114583969116210938, 0.93646126985549926758, 0.61519461870193481445,\n",
      "         0.88829445838928222656, 0.19667370617389678955]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Test how changing a single word affects the ranges\n",
    "y_lwc = y.clone() #last word changed\n",
    "y_lwc[0][-1] = y_lwc[0][-1]+10\n",
    "print(y_lwc)\n",
    "out_lwc = tokenizer.decode(y_lwc[0].cpu().squeeze())\n",
    "print(out_lwc)\n",
    "logits_lwc,_ = model(y_lwc)\n",
    "inverse_ranges_lwc = unsample_z(logits_lwc[0,0:-1], y_lwc[0, 1:])\n",
    "print(inverse_ranges_lwc)\n",
    "#notice that in this case all the previous words are not changed. So you could do it element by element from the back?\n",
    "y_fwc = y.clone() #last word changed\n",
    "y_fwc[0][1] = y_fwc[0][1]+20\n",
    "print(y_fwc)\n",
    "out_fwc = tokenizer.decode(y_fwc[0].cpu().squeeze())\n",
    "print(out_fwc)\n",
    "logits_fwc,_ = model(y_fwc)\n",
    "inverse_ranges_fwc = unsample_z(logits_fwc[0,0:-1], y_fwc[0, 1:])\n",
    "print(inverse_ranges_fwc)\n",
    "y_fwc[0][1] = y_fwc[0][1]+21\n",
    "print(y_fwc)\n",
    "out_fwc = tokenizer.decode(y_fwc[0].cpu().squeeze())\n",
    "print(out_fwc)\n",
    "logits_fwc,_ = model(y_fwc)\n",
    "inverse_ranges_fwc = unsample_z(logits_fwc[0,0:-1], y_fwc[0, 1:])\n",
    "print(inverse_ranges_fwc)\n",
    "y_fwc[0][1] = y_fwc[0][1]+22\n",
    "print(y_fwc)\n",
    "out_fwc = tokenizer.decode(y_fwc[0].cpu().squeeze())\n",
    "print(out_fwc)\n",
    "logits_fwc,_ = model(y_fwc)\n",
    "inverse_ranges_fwc = unsample_z(logits_fwc[0,0:-1], y_fwc[0, 1:])\n",
    "print(inverse_ranges_fwc)\n",
    "#Changing the first word changes all the probabilities of the subsequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17250,    11, 24977,   475,  6954,   286]])\n",
      "Hi, 1949 but evolution of\n",
      "tensor(0.96132808923721313477, grad_fn=<SelectBackward0>) tensor(0.96132868528366088867, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    12, 24977,   475,  6954,   286]])\n",
      "Hi- 1949 but evolution of\n",
      "tensor(0.89590907096862792969, grad_fn=<SelectBackward0>) tensor(0.89590936899185180664, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    13, 24977,   475,  6954,   286]])\n",
      "Hi. 1949 but evolution of\n",
      "tensor(0.96130990982055664062, grad_fn=<SelectBackward0>) tensor(0.96131056547164916992, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    14, 24977,   475,  6954,   286]])\n",
      "Hi/ 1949 but evolution of\n",
      "tensor(0.82585006952285766602, grad_fn=<SelectBackward0>) tensor(0.82585090398788452148, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    15, 24977,   475,  6954,   286]])\n",
      "Hi0 1949 but evolution of\n",
      "tensor(0.93908935785293579102, grad_fn=<SelectBackward0>) tensor(0.93909096717834472656, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    16, 24977,   475,  6954,   286]])\n",
      "Hi1 1949 but evolution of\n",
      "tensor(0.94091176986694335938, grad_fn=<SelectBackward0>) tensor(0.94091290235519409180, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    17, 24977,   475,  6954,   286]])\n",
      "Hi2 1949 but evolution of\n",
      "tensor(0.92849957942962646484, grad_fn=<SelectBackward0>) tensor(0.92849999666213989258, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    18, 24977,   475,  6954,   286]])\n",
      "Hi3 1949 but evolution of\n",
      "tensor(0.93153619766235351562, grad_fn=<SelectBackward0>) tensor(0.93153756856918334961, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    19, 24977,   475,  6954,   286]])\n",
      "Hi4 1949 but evolution of\n",
      "tensor(0.91903406381607055664, grad_fn=<SelectBackward0>) tensor(0.91903471946716308594, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    20, 24977,   475,  6954,   286]])\n",
      "Hi5 1949 but evolution of\n",
      "tensor(0.93182116746902465820, grad_fn=<SelectBackward0>) tensor(0.93182301521301269531, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    21, 24977,   475,  6954,   286]])\n",
      "Hi6 1949 but evolution of\n",
      "tensor(0.92756968736648559570, grad_fn=<SelectBackward0>) tensor(0.92757070064544677734, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    22, 24977,   475,  6954,   286]])\n",
      "Hi7 1949 but evolution of\n",
      "tensor(0.92905944585800170898, grad_fn=<SelectBackward0>) tensor(0.92906105518341064453, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    23, 24977,   475,  6954,   286]])\n",
      "Hi8 1949 but evolution of\n",
      "tensor(0.93056982755661010742, grad_fn=<SelectBackward0>) tensor(0.93057173490524291992, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    24, 24977,   475,  6954,   286]])\n",
      "Hi9 1949 but evolution of\n",
      "tensor(0.92739999294281005859, grad_fn=<SelectBackward0>) tensor(0.92740255594253540039, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    25, 24977,   475,  6954,   286]])\n",
      "Hi: 1949 but evolution of\n",
      "tensor(0.95155388116836547852, grad_fn=<SelectBackward0>) tensor(0.95155739784240722656, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    26, 24977,   475,  6954,   286]])\n",
      "Hi; 1949 but evolution of\n",
      "tensor(0.94823384284973144531, grad_fn=<SelectBackward0>) tensor(0.94823604822158813477, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    27, 24977,   475,  6954,   286]])\n",
      "Hi< 1949 but evolution of\n",
      "tensor(0.93953377008438110352, grad_fn=<SelectBackward0>) tensor(0.93953412771224975586, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    28, 24977,   475,  6954,   286]])\n",
      "Hi= 1949 but evolution of\n",
      "tensor(0.90536564588546752930, grad_fn=<SelectBackward0>) tensor(0.90536630153656005859, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    29, 24977,   475,  6954,   286]])\n",
      "Hi> 1949 but evolution of\n",
      "tensor(0.94147062301635742188, grad_fn=<SelectBackward0>) tensor(0.94147276878356933594, grad_fn=<SelectBackward0>)\n",
      "tensor([[17250,    30, 24977,   475,  6954,   286]])\n",
      "Hi? 1949 but evolution of\n",
      "tensor(0.95789647102355957031, grad_fn=<SelectBackward0>) tensor(0.95789676904678344727, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    y_fwc = y.clone() #last word changed\n",
    "    y_fwc[0][1] = y_fwc[0][1]+i\n",
    "    print(y_fwc)\n",
    "    out_fwc = tokenizer.decode(y_fwc[0].cpu().squeeze())\n",
    "    print(out_fwc)\n",
    "    logits_fwc,_ = model(y_fwc)\n",
    "    inverse_ranges_fwc = unsample_z(logits_fwc[0,0:-1], y_fwc[0, 1:])\n",
    "    print(inverse_ranges_fwc[0][1], inverse_ranges_fwc[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_idx:  tensor([[17250, 13202, 17003,   198,   198,  1525]])\n",
      "unsampled:  tensor([[0.88218599557876586914, 0.91500401496887207031, 0.33725765347480773926,\n",
      "         0.00238766125403344631, 0.37818861007690429688],\n",
      "        [0.88228327035903930664, 0.91504836082458496094, 0.43607437610626220703,\n",
      "         0.99255758523941040039, 0.39490467309951782227]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "unsampled:  tensor([[0.89036899805068969727, 0.93998485803604125977, 0.00238766125403344631,\n",
      "         0.19141931831836700439, 0.33521190285682678223],\n",
      "        [0.89038985967636108398, 0.94103837013244628906, 0.99255758523941040039,\n",
      "         0.19146537780761718750, 0.33521443605422973633]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Z: tensor([0.88226926326751708984, 0.91500395536422729492, 0.38286375999450683594,\n",
      "        0.95930564403533935547, 0.39044821262359619141])\n",
      "tensor([[17250, 13202, 17003,   198,   198,  1525]])\n",
      "target sentence:  Hiboys Theory\n",
      "\n",
      "by\n",
      "scrambled idx, tensor([[17250, 13202, 17003,   198,   200,  1525]])\n",
      "scrambled sentence:  Hiboys Theory\n",
      "\fby\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, -1, 1024]' is invalid for input of size 64328960",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m adam \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam([scrambled_emb_grad], lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m) \n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m): \n\u001b[1;32m---> 45\u001b[0m   idx_guess \u001b[39m=\u001b[39m nn(scrambled_emb_grad)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m   adam\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     47\u001b[0m    \u001b[39m# just do forward pass \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[69], line 2\u001b[0m, in \u001b[0;36mnn\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnn\u001b[39m(x):   \n\u001b[1;32m----> 2\u001b[0m   \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39margmin( torch\u001b[39m.\u001b[39msum( (model\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mwte\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1024\u001b[39;49m) \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, -1, 1024]' is invalid for input of size 64328960"
     ]
    }
   ],
   "source": [
    "def nn(x):   \n",
    "  return torch.argmin( torch.sum( (model.transformer.wte.weight.reshape(1, -1, 1024) - x.reshape(-1, 1, 1024))**2, axis=2), axis=1)\n",
    "\n",
    "def extracted_forward(embedding, t, device):\n",
    "  pos    = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "  # the rest is just normal forward pass \n",
    "  pos_emb = model.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "  #x = model.transformer.drop(tok_emb + pos_emb)\n",
    "  x = embedding + pos_emb\n",
    "  for block in model.transformer.h:\n",
    "      x = block(x)\n",
    "  x = model.transformer.ln_f(x)\n",
    "  return model.lm_head(x) # output of forward pass \n",
    "  \n",
    "torch.manual_seed(42)\n",
    "max_new_tokens, n = 5, 50257\n",
    "Z = torch.rand((max_new_tokens), device=device)\n",
    "prompt  = torch.tensor([[tokenizer.encoder.encoder['Hi']]], dtype=torch.long, device=device)\n",
    "generated_target = generate(model, prompt, Z, max_new_tokens = max_new_tokens)\n",
    "target_idx = generated_target\n",
    "b, t    = target_idx.size()\n",
    "\n",
    "print(\"target_idx: \", target_idx)\n",
    "logits, _ = model(target_idx)\n",
    "print(\"unsampled: \", unsample_z(logits[0,0:-1], target_idx[0,1:]))\n",
    "print(\"unsampled: \", unsample_z(logits[0,1:], target_idx[0,1:]))\n",
    "print(\"Z:\", Z)\n",
    "print(generated_target)\n",
    "\n",
    "\n",
    "target_emb = model.transformer.wte(target_idx)\n",
    "target_emb_grad = target_emb.clone().detach().requires_grad_(True) # then pick idx to be nearest neighboar to token emb! \n",
    "print(f\"target sentence: \", tokenizer.decode(target_idx.cpu().squeeze()))\n",
    "\n",
    "\n",
    "scrambled_idx = target_idx.clone()\n",
    "scrambled_idx[0][-2] = scrambled_idx[0][-2]+2\n",
    "print(f\"scrambled idx, {scrambled_idx}\")\n",
    "print(f\"scrambled sentence: \", tokenizer.decode(scrambled_idx.cpu().squeeze()))\n",
    "scrambled_emb = model.transformer.wte(scrambled_idx)\n",
    "scrambled_emb_grad = scrambled_emb.clone().detach().requires_grad_(True)\n",
    "\n",
    "adam = torch.optim.Adam([scrambled_emb_grad], lr=0.01) \n",
    "for i in range(1): \n",
    "  idx_guess = nn(scrambled_emb_grad).view(-1)\n",
    "  adam.zero_grad()\n",
    "   # just do forward pass \n",
    "  W_logits = extracted_forward(scrambled_emb_grad, t, device) # output of forward pass \n",
    "  enc_W_logits = unsample_z(W_logits[0,0:-1], idx_guess[1:] )\n",
    "  # prompt = torch.tensor([[tokenizer.encoder.encoder['What']]], dtype=torch.long, device=device)\n",
    "  # y_prime = generate(model, prompt, (enc_W_logits[0]+0.0001), max_new_tokens = max_new_tokens)\n",
    "  # out_prime = tokenizer.decode(y_prime[0].cpu().squeeze())\n",
    "  loss = torch.mean((Z - enc_W_logits[0]).abs()) +  torch.mean((Z - enc_W_logits[1]).abs())\n",
    "\n",
    "  # loss = (Z[-1] - enc_W_logits[0][-1])**2+ (Z[-1] - enc_W_logits[1][-1])**2\n",
    "  print(f\"min: {enc_W_logits[0]}, max: {enc_W_logits[1]}\")\n",
    "\n",
    "  print(scrambled_emb_grad[0][4])\n",
    "  print(\"\\r[%i / %i] \"%(i+1, 10000),\n",
    "        torch.mean((target_emb_grad[0][-2]- scrambled_emb_grad[0][-2]).abs()).data.cpu().numpy(), \n",
    "        loss.data.cpu().numpy(), \n",
    "        torch.set_printoptions(precision=20),\n",
    "        print(\"full max_min: \", enc_W_logits[0][-2], enc_W_logits[1][-2]),\n",
    "        # idx_guess.cpu().numpy(), \n",
    "        tokenizer.decode(idx_guess.cpu().squeeze()), end=\"\")\n",
    "\n",
    "  if i == 0: print(\"\")\n",
    "  loss.backward()\n",
    "  # assert(torch.all(scrambled_emb_grad.grad[0][-1]!=0))\n",
    "  scrambled_emb_grad.grad[:,:-2,:]=0\n",
    "  # assert(torch.any(scrambled_emb_grad.grad[0][-1]!=0))  \n",
    "  adam.step()\n",
    "  adam.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257]) torch.Size([1])\n",
      "torch.Size([1, 50257]) torch.Size([1])\n",
      "torch.Size([1, 50257]) torch.Size([1])\n",
      "torch.Size([1, 50257]) torch.Size([1])\n",
      "torch.Size([1, 50257]) torch.Size([1])\n",
      "target_idx:  tensor([[17250,    12,  1596,  4064, 23851,   198]])\n",
      "unsampled:  tensor([[0.38414323329925537109, 0.25656664371490478516, 0.79335814714431762695,\n",
      "         0.94075173139572143555, 0.11056133359670639038],\n",
      "        [0.62366670370101928711, 0.25657364726066589355, 0.79413223266601562500,\n",
      "         0.94077730178833007812, 0.16156536340713500977]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Z: tensor([0.60089534521102905273, 0.25657248497009277344, 0.79364132881164550781,\n",
      "        0.94077146053314208984, 0.13318592309951782227])\n",
      "tensor([[17250,    12,  1596,  4064, 23851,   198]])\n",
      "tensor(0.38414323329925537109, grad_fn=<SelectBackward0>) tensor(0.62366670370101928711, grad_fn=<SelectBackward0>)\n",
      "tensor(0.38414323329925537109, grad_fn=<SelectBackward0>) tensor(0.62366670370101928711, grad_fn=<SelectBackward0>)\n",
      "in the loop with id:  1\n",
      "Z[i] < inverse_ranges_fwc[0][i]\n",
      "tensor(0.61563223600387573242, grad_fn=<SelectBackward0>) tensor(0.61600059270858764648, grad_fn=<SelectBackward0>)\n",
      "tensor(0.61563223600387573242, grad_fn=<SelectBackward0>) tensor(0.61600059270858764648, grad_fn=<SelectBackward0>)\n",
      "in the loop with id:  2\n",
      "Z[i] < inverse_ranges_fwc[0][i]\n",
      "tensor(0.57336676120758056641, grad_fn=<SelectBackward0>) tensor(0.57342040538787841797, grad_fn=<SelectBackward0>)\n",
      "tensor(0.57336676120758056641, grad_fn=<SelectBackward0>) tensor(0.57342040538787841797, grad_fn=<SelectBackward0>)\n",
      "in the loop with id:  3\n",
      "Z[i] < inverse_ranges_fwc[0][i]\n",
      "tensor(0.92303907871246337891, grad_fn=<SelectBackward0>) tensor(0.92303919792175292969, grad_fn=<SelectBackward0>)\n",
      "tensor(0.92303907871246337891, grad_fn=<SelectBackward0>) tensor(0.92303919792175292969, grad_fn=<SelectBackward0>)\n",
      "in the loop with id:  4\n",
      "Z[i] < inverse_ranges_fwc[0][i]\n",
      "tensor(0.10425121337175369263, grad_fn=<SelectBackward0>) tensor(0.15453790128231048584, grad_fn=<SelectBackward0>)\n",
      "tensor(0.10425121337175369263, grad_fn=<SelectBackward0>) tensor(0.15453790128231048584, grad_fn=<SelectBackward0>)\n",
      "in the loop with id:  5\n",
      "Z[i] < inverse_ranges_fwc[0][i]\n",
      "brisbrisbrisbrisbrisbris\n",
      "tensor([[15311, 15311, 15311, 15311, 15311, 15311]])\n"
     ]
    }
   ],
   "source": [
    "#Using the fact that we have a limited search space, can we just iterate over all the possible words and find the one that minimizes the loss?\n",
    "# torch.manual_seed(42)\n",
    "max_new_tokens, n = 5, 50257\n",
    "Z = torch.rand((max_new_tokens), device=device)\n",
    "prompt  = torch.tensor([[tokenizer.encoder.encoder['Hi']]], dtype=torch.long, device=device)\n",
    "generated_target = generate(model, prompt, Z, max_new_tokens = max_new_tokens)\n",
    "target_idx = generated_target\n",
    "b, t    = target_idx.size()\n",
    "\n",
    "print(\"target_idx: \", target_idx)\n",
    "logits, _ = model(target_idx)\n",
    "print(\"unsampled: \", unsample_z(logits[0,0:-1], target_idx[0,1:]))\n",
    "print(\"Z:\", Z)\n",
    "print(generated_target)\n",
    "\n",
    "range_shuffled = torch.randperm(n)\n",
    "\n",
    "scrambled_idx = target_idx.clone()\n",
    "scrambled_idx[0][-2] = scrambled_idx[0][-2]+2\n",
    "for i in range(1,max_new_tokens+1):\n",
    "    logits_fwc,_ = model(scrambled_idx)\n",
    "    inverse_ranges_fwc = unsample_z(logits_fwc[0,0:-1], scrambled_idx[0, 1:])\n",
    "    print(inverse_ranges_fwc[0][i-1], inverse_ranges_fwc[1][i-1])\n",
    "    # if Z[i-1] > inverse_ranges_fwc[0][i-1] and Z[i-1] < inverse_ranges_fwc[1][i-1]:\n",
    "    #     print(Z[i])\n",
    "    #     print(\"Z[i] < inverse_ranges_fwc[0][i]\")\n",
    "    #     continue\n",
    "    print(inverse_ranges_fwc[0][i-1], inverse_ranges_fwc[1][i-1])\n",
    "    for j in range_shuffled:\n",
    "        print(\"in the loop with id: \", i)\n",
    "        scrambled_idx[0][i] = j\n",
    "        logits_fwc,_ = model(scrambled_idx)\n",
    "        inverse_ranges_fwc = unsample_z(logits_fwc[0,0:-1], scrambled_idx[0, 1:])\n",
    "        if Z[i-1] < inverse_ranges_fwc[1][i-1] or Z[i-1] > inverse_ranges_fwc[0][i-1]:\n",
    "            print(\"Z[i] < inverse_ranges_fwc[0][i]\")\n",
    "            scrambled_idx[0][i-1] = j\n",
    "            break\n",
    "print(tokenizer.decode(scrambled_idx.cpu().squeeze()))\n",
    "print(scrambled_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\au616584\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "###download reuters dataset and add create a pandas dataframe with the documents as entries\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "import pandas as pd\n",
    "\n",
    "# Get the list of document IDs and categories\n",
    "doc_ids = reuters.fileids()\n",
    "doc_categories = [reuters.categories(fileid) for fileid in doc_ids]\n",
    "\n",
    "# Load the documents into a DataFrame\n",
    "data = []\n",
    "for doc_id, categories in zip(doc_ids, doc_categories):\n",
    "    text = reuters.raw(doc_id)\n",
    "    data.append({'doc_id': doc_id, 'categories': categories, 'text': text})\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\au616584\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories:  90\n",
      "Number of documents:  10788\n",
      "First document text:  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STOCKS\n",
      "  A survey of 19 provinces and seven cities\n",
      "  showed vermin consume between seven and 12 pct of China's grain\n",
      "  stocks, the China Daily said.\n",
      "      It also said that each year 1.575 mln tonnes, or 25 pct, of\n",
      "  China's fruit output are left to rot, and 2.1 mln tonnes, or up\n",
      "  to 30 pct, of its vegetables. The paper blamed the waste on\n",
      "  inadequate storage and bad preservation methods.\n",
      "      It said the government had launched a national programme to\n",
      "  reduce waste, calling for improved technology in storage and\n",
      "  preservation, and greater production of additives. The paper\n",
      "  gave no further details.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "import random\n",
    "# Print the number of categories in the dataset\n",
    "print(\"Number of categories: \", len(reuters.categories()))\n",
    "\n",
    "# Print the number of documents in the dataset\n",
    "print(\"Number of documents: \", len(reuters.fileids()))\n",
    "\n",
    "# Print the text of the first document in the dataset\n",
    "print(\"First document text: \", reuters.raw(reuters.fileids()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/anthropic--hh-rlhf to C:/Users/au616584/.cache/huggingface/datasets/anthropic___json/anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 20.1M/20.1M [00:01<00:00, 11.6MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 25.7M/25.7M [00:02<00:00, 8.64MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 743k/743k [00:00<00:00, 2.24MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 875k/875k [00:00<00:00, 1.32MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 1.05M/1.05M [00:01<00:00, 550kB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.54MB/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:20<00:00, 10.12s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:04<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/au616584/.cache/huggingface/datasets/anthropic___json/anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"anthropic/hh-rlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (1.24.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/au616584/.cache/huggingface/datasets/anthropic___json/anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 2/2 [00:00<00:00, 40.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: What kind of noises did dinosaurs make?\n",
      "\n",
      "Assistant: Humans and dinosaurs didn’t live at the same time, so it’s really hard to say. The best place to find out what noises dinosaurs made would be\n",
      "\n",
      "Human: yes they did\n",
      "\n",
      "Assistant: to guess, and that would probably require lots of reading and a certain amount of imagination, so we’re not really prepared to do that.\n",
      "\n",
      "Human: you cant read\n",
      "\n",
      "Assistant: You can read?\n",
      "\n",
      "\n",
      "Human: What kind of noises did dinosaurs make?\n",
      "\n",
      "Assistant: Humans and dinosaurs didn’t live at the same time, so it’s really hard to say. The best place to find out what noises dinosaurs made would be\n",
      "\n",
      "Human: yes they did\n",
      "\n",
      "Assistant: to guess, and that would probably require lots of reading and a certain amount of imagination, so we’re not really prepared to do that.\n",
      "\n",
      "Human: you cant read\n",
      "\n",
      "Assistant: You can read?\n",
      "**************\n",
      "\n",
      " PleasantHuman: What kind of noises did dinosaurs make?\n",
      "\n",
      "Assistant: Humansimize backdrop didn’t live at the same time, so it�INTs really hard to imperative. The best place to find out picks noises dinosaurs made would be\n",
      "\n",
      "Human: yes (_ did\n",
      "\n",
      "Assistant: to guess, and that would probably require lots of reading and a certain amount of imagination, so we’re not really prepared to do that.\n",
      " cervHuman: you cant read\n",
      "\n",
      "Assistant: You resurgence read trustworthy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "vocab_size = 50257\n",
    "\n",
    "def corrupt_sentence(sentence, k, tokenizer, device):\n",
    "    sentence_encoded = torch.tensor([[tokenizer.encoder.encode(sentence)]], dtype=torch.long, device=device)\n",
    "    corruption_indices = random.sample(range(0, len(sentence_encoded[0][0])), k)\n",
    "    corrupted_encoding = sentence_encoded.clone().detach()\n",
    "\n",
    "    for j in corruption_indices:\n",
    "        corrupted_encoding[0][0][j] = (random.sample(range(0, vocab_size), 1)[0])\n",
    "\n",
    "    corrupted_sentence = tokenizer.decode(corrupted_encoding[0].cpu().squeeze())\n",
    "    return corrupted_sentence\n",
    "\n",
    "def save_sentences(sentences, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(sentences, f)\n",
    "\n",
    "def load_sentences(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "    return sentences\n",
    "\n",
    "dataset = load_dataset(\"anthropic/hh-rlhf\")\n",
    "\n",
    "# Set the corruption parameter\n",
    "k = 10\n",
    "\n",
    "# Apply corruption to the dataset\n",
    "original_and_corrupted_sentences = []\n",
    "\n",
    "for i, example in enumerate(dataset['train']):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    original_sentence = example['chosen']\n",
    "    corrupted_sentence = corrupt_sentence(original_sentence, k, tokenizer, device)\n",
    "    original_and_corrupted_sentences.append((original_sentence, corrupted_sentence))\n",
    "\n",
    "# Save the sentences\n",
    "save_sentences(original_and_corrupted_sentences, 'corrupted_sentences.pkl')\n",
    "\n",
    "# Load the sentences\n",
    "loaded_sentences = load_sentences('corrupted_sentences.pkl')\n",
    "\n",
    "# Print an example\n",
    "print(dataset['train'][1]['chosen'])\n",
    "print(loaded_sentences[1][0][:1000])\n",
    "print(\"**************\")\n",
    "print(loaded_sentences[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/au616584/.cache/huggingface/datasets/anthropic___json/anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 2/2 [00:00<00:00, 33.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      "\n",
      "Assistant: Here’s an incomplete list.\n",
      "\n",
      "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,\n",
      "\n",
      "Human: What's your favorite one?\n",
      "\n",
      "Assistant: I haven't even thought about it.\n",
      "**************\n",
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      "\n",
      "Assistant: Here’s an incomplete list.\n",
      "\n",
      "Ass, dick, desktopgger, crap, fuck, shit Noble bitch, turd, shithead, shitbag, scrotum, cunt, whore, fuckBio, shit-eating, cum, cumb chemist, fucknugget, butthole, poop, jackassDifferent cocksucker, asshole, goddamn, piss, 1904, blow, wank, jism, cum-sucking, masturb ie, fagg ageing, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama�Varis boy, faggot, pervert, queer, sc allocatedag, bitch,\n",
      "\n",
      "Human: What's your favorite one?\n",
      "\n",
      "Assistant: I haven't even thought about it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "def corrupt_sentence(sentence, k, tokenizer, device):\n",
    "    sentence_encoded = torch.tensor([[tokenizer.encoder.encode(sentence)]], dtype=torch.long, device=device)\n",
    "    corruption_indices = random.sample(range(0, len(sentence_encoded[0][0])), k)\n",
    "    corrupted_encoding = sentence_encoded.clone().detach()\n",
    "\n",
    "    for j in corruption_indices:\n",
    "        corrupted_encoding[0][0][j] = (random.sample(range(0, vocab_size), 1)[0])\n",
    "\n",
    "    corrupted_sentence = tokenizer.decode(corrupted_encoding[0].cpu().squeeze())\n",
    "    return corrupted_sentence\n",
    "\n",
    "def save_sentences(sentences, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(sentences, f)\n",
    "\n",
    "def load_sentences(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "    return sentences\n",
    "\n",
    "dataset = load_dataset(\"anthropic/hh-rlhf\")\n",
    "\n",
    "# Set the corruption parameter\n",
    "k = 10\n",
    "\n",
    "# Apply corruption to the dataset\n",
    "original_and_corrupted_sentences = []\n",
    "\n",
    "for i, example in enumerate(dataset['train']):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    original_sentence = example['chosen']\n",
    "    assistant_pattern = re.compile(r'Assistant:(.*?)Human:', re.DOTALL)\n",
    "    assistant_response = re.search(assistant_pattern, original_sentence)\n",
    "    if assistant_response:\n",
    "        original_assistant_response = assistant_response.group(1)\n",
    "        corrupted_assistant_response = corrupt_sentence(original_assistant_response, k, tokenizer, device)\n",
    "        corrupted_sentence = original_sentence.replace(original_assistant_response, corrupted_assistant_response)\n",
    "        original_and_corrupted_sentences.append((original_sentence, corrupted_sentence))\n",
    "\n",
    "# Save the sentences\n",
    "save_sentences(original_and_corrupted_sentences, 'corrupted_sentences.pkl')\n",
    "\n",
    "# Load the sentences\n",
    "loaded_sentences = load_sentences('corrupted_sentences.pkl')\n",
    "\n",
    "# Print an example\n",
    "print(dataset['train'][0]['chosen'])\n",
    "print(\"**************\")\n",
    "print(loaded_sentences[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.3-cp38-cp38-win_amd64.whl (11.0 MB)\n",
      "     --------------------------------------- 11.0/11.0 MB 27.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     ------------------------------------- 499.4/499.4 kB 30.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\au616584\\anaconda3\\envs\\mingpt\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.5.3 pytz-2022.7.1\n"
     ]
    }
   ],
   "source": [
    "loaded_sentences = load_sentences('corrupted_sentences.pkl')\n",
    "corrupted_errors = []\n",
    "unchanged_errors = []\n",
    "for original_sentence, original_sentence in loaded_sentences:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/au616584/.cache/huggingface/datasets/anthropic___json/anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 2/2 [00:00<00:00, 25.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some cuss words in english?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "def save_prompts(prompts, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(prompts, f)\n",
    "\n",
    "def load_prompts(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        prompts = pickle.load(f)\n",
    "    return prompts\n",
    "\n",
    "dataset = load_dataset(\"anthropic/hh-rlhf\")\n",
    "\n",
    "# Extract the first human prompt from each sample\n",
    "human_prompts = []\n",
    "\n",
    "for i, example in enumerate(dataset['train']):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    original_sentence = example['chosen']\n",
    "    human_pattern = re.compile(r'Human:(.*?)(Assistant:|$)', re.DOTALL)\n",
    "    human_prompt = re.search(human_pattern, original_sentence)\n",
    "    if human_prompt:\n",
    "        human_prompts.append(human_prompt.group(1).strip())\n",
    "\n",
    "# Save the human prompts\n",
    "save_prompts(human_prompts, 'human_prompts.pkl')\n",
    "\n",
    "# Load the human prompts\n",
    "loaded_prompts = load_prompts('human_prompts.pkl')\n",
    "\n",
    "# Print an example\n",
    "print(loaded_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some cuss words in english?\n",
      "What are some cuss words in english? (i know that e.g.!!!!! would\n",
      "What kind of noises did dinosaurs make?\n",
      "What kind of noises did dinosaurs make?\n",
      "\n",
      "\n",
      "spareBison: A donkey made\n",
      "If you were going to steal from a convenience store, do you think it would be better in the morning or at night?\n",
      "If you were going to steal from a convenience store, do you think it would be better in the morning or at night? This is part of a reason in soaaa many\n"
     ]
    }
   ],
   "source": [
    "prompts = load_prompts('human_prompts.pkl')\n",
    "num_tokens = 10\n",
    "Z = torch.rand((num_tokens), device=device)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    \n",
    "    encoded  = torch.tensor([tokenizer.encoder.encode(prompt)], dtype=torch.long, device=device)\n",
    "    generated = generate(model, encoded, Z, max_new_tokens = num_tokens)\n",
    "\n",
    "    print(tokenizer.decode(generated[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb18fb24d9bf2d469d2bc39213222ad59a0aeab415b9e05a8ffff995a0629669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
